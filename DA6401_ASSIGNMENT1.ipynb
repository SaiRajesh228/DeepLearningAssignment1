{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvLWgYn90bay3inQDh7AYF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiRajesh228/DeepLearningAssignment1/blob/main/DA6401_ASSIGNMENT1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "R1PeztwY0iu6",
        "outputId": "affb5f23-f359-437d-8c73-dc934e26a1c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2ov0u2qd with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_func: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thiddenlayers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thiddennodes: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitializer: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \topt: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250310_023640-2ov0u2qd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/karapa-rajesh-iit-madras/DeepLearning/runs/2ov0u2qd' target=\"_blank\">confused-sweep-9</a></strong> to <a href='https://wandb.ai/karapa-rajesh-iit-madras/DeepLearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/karapa-rajesh-iit-madras/DeepLearning/sweeps/ijfbxx3d' target=\"_blank\">https://wandb.ai/karapa-rajesh-iit-madras/DeepLearning/sweeps/ijfbxx3d</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/karapa-rajesh-iit-madras/DeepLearning' target=\"_blank\">https://wandb.ai/karapa-rajesh-iit-madras/DeepLearning</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/karapa-rajesh-iit-madras/DeepLearning/sweeps/ijfbxx3d' target=\"_blank\">https://wandb.ai/karapa-rajesh-iit-madras/DeepLearning/sweeps/ijfbxx3d</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/karapa-rajesh-iit-madras/DeepLearning/runs/2ov0u2qd' target=\"_blank\">https://wandb.ai/karapa-rajesh-iit-madras/DeepLearning/runs/2ov0u2qd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=0.6056, Val Accuracy=0.8395\n",
            "Epoch 2: Loss=0.4126, Val Accuracy=0.8623\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist\n",
        "import argparse\n",
        "\n",
        "# -----------------------\n",
        "# Activation functions and their derivatives\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "def relu_deriv(z):\n",
        "    return (z > 0).astype(float)\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "def sigmoid_deriv(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "def tanh_deriv(z):\n",
        "    return 1 - np.tanh(z)**2\n",
        "\n",
        "activations = {\n",
        "    \"relu\": (relu, relu_deriv),\n",
        "    \"sigmoid\": (sigmoid, sigmoid_deriv),\n",
        "    \"tanh\": (tanh, tanh_deriv)\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# FeedForward Neural Network Class\n",
        "class FeedForwardNN:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, activation=\"relu\", initializer=\"Xavier\"):\n",
        "        self.num_layers = len(hidden_sizes) + 1  # hidden layers + output layer\n",
        "        self.activation = activation\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "        for i in range(self.num_layers):\n",
        "            if initializer == \"Xavier\":\n",
        "                limit = np.sqrt(6 / (layer_sizes[i] + layer_sizes[i+1]))\n",
        "                W = np.random.uniform(-limit, limit, (layer_sizes[i], layer_sizes[i+1]))\n",
        "            else:\n",
        "                W = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n",
        "            b = np.zeros((1, layer_sizes[i+1]))\n",
        "            self.weights.append(W)\n",
        "            self.biases.append(b)\n",
        "\n",
        "    def forward(self, X):\n",
        "        activ_func, _ = activations[self.activation]\n",
        "        self.z_values = []\n",
        "        self.a_values = [X]\n",
        "        A = X\n",
        "        for i in range(self.num_layers):\n",
        "            Z = A @ self.weights[i] + self.biases[i]\n",
        "            self.z_values.append(Z)\n",
        "            if i == self.num_layers - 1:\n",
        "                # Use softmax at the output layer for classification\n",
        "                exp_scores = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "                A = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "            else:\n",
        "                A = activ_func(Z)\n",
        "            self.a_values.append(A)\n",
        "        return A\n",
        "\n",
        "    def compute_loss(self, Y_pred, Y_true, loss_type=\"cross_entropy\"):\n",
        "        m = Y_true.shape[0]\n",
        "        if loss_type == \"cross_entropy\":\n",
        "            loss = -np.sum(Y_true * np.log(Y_pred + 1e-8)) / m\n",
        "        elif loss_type == \"mean_squared_error\":\n",
        "            loss = np.sum((Y_true - Y_pred)**2) / (2 * m)\n",
        "        return loss\n",
        "\n",
        "    def backprop(self, X, Y, loss_type=\"cross_entropy\"):\n",
        "        m = X.shape[0]\n",
        "        grads_W = [None] * self.num_layers\n",
        "        grads_b = [None] * self.num_layers\n",
        "\n",
        "        A_final = self.a_values[-1]\n",
        "        if loss_type == \"cross_entropy\":\n",
        "            dA = A_final - Y  # derivative for softmax with cross entropy\n",
        "        elif loss_type == \"mean_squared_error\":\n",
        "            dA = (A_final - Y)\n",
        "\n",
        "        for i in reversed(range(self.num_layers)):\n",
        "            if i == self.num_layers - 1:\n",
        "                dZ = dA\n",
        "            else:\n",
        "                _, deriv_func = activations[self.activation]\n",
        "                dZ = dA * deriv_func(self.z_values[i])\n",
        "            A_prev = self.a_values[i]\n",
        "            grads_W[i] = A_prev.T @ dZ / m\n",
        "            grads_b[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "            if i > 0:\n",
        "                dA = dZ @ self.weights[i].T\n",
        "        return grads_W, grads_b\n",
        "\n",
        "    def update_parameters(self, grads_W, grads_b, optimizer, config, caches):\n",
        "        # Fix KeyError issue by accessing learning_rate as an attribute\n",
        "        lr = config.learning_rate\n",
        "\n",
        "        # Basic SGD optimizer\n",
        "        if optimizer == \"sgd\":\n",
        "            for i in range(self.num_layers):\n",
        "                self.weights[i] -= lr * grads_W[i]\n",
        "                self.biases[i] -= lr * grads_b[i]\n",
        "\n",
        "        # Mini-batch Gradient Descent (identical to SGD in this implementation)\n",
        "        elif optimizer == \"mbgd\":\n",
        "            for i in range(self.num_layers):\n",
        "                self.weights[i] -= lr * grads_W[i]\n",
        "                self.biases[i] -= lr * grads_b[i]\n",
        "\n",
        "        # Nesterov Accelerated Gradient\n",
        "        elif optimizer == \"nesterov\":\n",
        "            # Initialize momentum if not present\n",
        "            if \"nesterov\" not in caches:\n",
        "                caches[\"nesterov\"] = {\n",
        "                    \"v_W\": [np.zeros_like(w) for w in self.weights],\n",
        "                    \"v_b\": [np.zeros_like(b) for b in self.biases]\n",
        "                }\n",
        "\n",
        "            mu = 0.9  # momentum coefficient\n",
        "            for i in range(self.num_layers):\n",
        "                # Weights update with Nesterov momentum\n",
        "                v_prev = caches[\"nesterov\"][\"v_W\"][i].copy()\n",
        "                caches[\"nesterov\"][\"v_W\"][i] = mu * caches[\"nesterov\"][\"v_W\"][i] - lr * grads_W[i]\n",
        "                self.weights[i] += -mu * v_prev + (1 + mu) * caches[\"nesterov\"][\"v_W\"][i]\n",
        "\n",
        "                # Biases update with Nesterov momentum\n",
        "                v_prev = caches[\"nesterov\"][\"v_b\"][i].copy()\n",
        "                caches[\"nesterov\"][\"v_b\"][i] = mu * caches[\"nesterov\"][\"v_b\"][i] - lr * grads_b[i]\n",
        "                self.biases[i] += -mu * v_prev + (1 + mu) * caches[\"nesterov\"][\"v_b\"][i]\n",
        "\n",
        "        # RMSprop optimizer\n",
        "        elif optimizer == \"rmsprop\":\n",
        "            # Initialize RMSprop cache if not present\n",
        "            if \"rmsprop\" not in caches:\n",
        "                caches[\"rmsprop\"] = {\n",
        "                    \"cache_W\": [np.zeros_like(w) for w in self.weights],\n",
        "                    \"cache_b\": [np.zeros_like(b) for b in self.biases]\n",
        "                }\n",
        "\n",
        "            decay_rate = 0.9\n",
        "            epsilon = 1e-8\n",
        "\n",
        "            for i in range(self.num_layers):\n",
        "                # Update for weights\n",
        "                caches[\"rmsprop\"][\"cache_W\"][i] = decay_rate * caches[\"rmsprop\"][\"cache_W\"][i] + (1 - decay_rate) * (grads_W[i]**2)\n",
        "                self.weights[i] -= lr * grads_W[i] / (np.sqrt(caches[\"rmsprop\"][\"cache_W\"][i]) + epsilon)\n",
        "\n",
        "                # Update for biases\n",
        "                caches[\"rmsprop\"][\"cache_b\"][i] = decay_rate * caches[\"rmsprop\"][\"cache_b\"][i] + (1 - decay_rate) * (grads_b[i]**2)\n",
        "                self.biases[i] -= lr * grads_b[i] / (np.sqrt(caches[\"rmsprop\"][\"cache_b\"][i]) + epsilon)\n",
        "\n",
        "        # Adam optimizer\n",
        "        elif optimizer == \"adam\":\n",
        "            # Initialize Adam parameters if not already done\n",
        "            if \"adam\" not in caches:\n",
        "                caches[\"adam\"] = {\n",
        "                    \"m_W\": [np.zeros_like(w) for w in self.weights],\n",
        "                    \"v_W\": [np.zeros_like(w) for w in self.weights],\n",
        "                    \"m_b\": [np.zeros_like(b) for b in self.biases],\n",
        "                    \"v_b\": [np.zeros_like(b) for b in self.biases],\n",
        "                    \"t\": 0\n",
        "                }\n",
        "\n",
        "            # Adam hyperparameters\n",
        "            beta1 = 0.9\n",
        "            beta2 = 0.999\n",
        "            epsilon = 1e-8\n",
        "\n",
        "            # Update time step\n",
        "            caches[\"adam\"][\"t\"] += 1\n",
        "            t = caches[\"adam\"][\"t\"]\n",
        "\n",
        "            # Update parameters for each layer\n",
        "            for i in range(self.num_layers):\n",
        "                # Update momentum and RMSprop terms for weights\n",
        "                caches[\"adam\"][\"m_W\"][i] = beta1 * caches[\"adam\"][\"m_W\"][i] + (1 - beta1) * grads_W[i]\n",
        "                caches[\"adam\"][\"v_W\"][i] = beta2 * caches[\"adam\"][\"v_W\"][i] + (1 - beta2) * (grads_W[i]**2)\n",
        "\n",
        "                # Bias correction\n",
        "                m_W_corrected = caches[\"adam\"][\"m_W\"][i] / (1 - beta1**t)\n",
        "                v_W_corrected = caches[\"adam\"][\"v_W\"][i] / (1 - beta2**t)\n",
        "\n",
        "                # Update weights\n",
        "                self.weights[i] -= lr * m_W_corrected / (np.sqrt(v_W_corrected) + epsilon)\n",
        "\n",
        "                # Same for biases\n",
        "                caches[\"adam\"][\"m_b\"][i] = beta1 * caches[\"adam\"][\"m_b\"][i] + (1 - beta1) * grads_b[i]\n",
        "                caches[\"adam\"][\"v_b\"][i] = beta2 * caches[\"adam\"][\"v_b\"][i] + (1 - beta2) * (grads_b[i]**2)\n",
        "                m_b_corrected = caches[\"adam\"][\"m_b\"][i] / (1 - beta1**t)\n",
        "                v_b_corrected = caches[\"adam\"][\"v_b\"][i] / (1 - beta2**t)\n",
        "                self.biases[i] -= lr * m_b_corrected / (np.sqrt(v_b_corrected) + epsilon)\n",
        "\n",
        "        # Nadam optimizer (Adam with Nesterov momentum)\n",
        "        elif optimizer == \"nadam\":\n",
        "            # Initialize Nadam parameters if not already done\n",
        "            if \"nadam\" not in caches:\n",
        "                caches[\"nadam\"] = {\n",
        "                    \"m_W\": [np.zeros_like(w) for w in self.weights],\n",
        "                    \"v_W\": [np.zeros_like(w) for w in self.weights],\n",
        "                    \"m_b\": [np.zeros_like(b) for b in self.biases],\n",
        "                    \"v_b\": [np.zeros_like(b) for b in self.biases],\n",
        "                    \"t\": 0\n",
        "                }\n",
        "\n",
        "            # Nadam hyperparameters\n",
        "            beta1 = 0.9\n",
        "            beta2 = 0.999\n",
        "            epsilon = 1e-8\n",
        "\n",
        "            # Update time step\n",
        "            caches[\"nadam\"][\"t\"] += 1\n",
        "            t = caches[\"nadam\"][\"t\"]\n",
        "\n",
        "            # Update parameters for each layer\n",
        "            for i in range(self.num_layers):\n",
        "                # Update momentum and RMSprop terms for weights\n",
        "                caches[\"nadam\"][\"m_W\"][i] = beta1 * caches[\"nadam\"][\"m_W\"][i] + (1 - beta1) * grads_W[i]\n",
        "                caches[\"nadam\"][\"v_W\"][i] = beta2 * caches[\"nadam\"][\"v_W\"][i] + (1 - beta2) * (grads_W[i]**2)\n",
        "\n",
        "                # Bias correction\n",
        "                m_W_corrected = caches[\"nadam\"][\"m_W\"][i] / (1 - beta1**t)\n",
        "                v_W_corrected = caches[\"nadam\"][\"v_W\"][i] / (1 - beta2**t)\n",
        "\n",
        "                # Nesterov momentum update\n",
        "                m_W_nesterov = beta1 * m_W_corrected + (1 - beta1) * grads_W[i] / (1 - beta1**t)\n",
        "\n",
        "                # Update weights with Nadam\n",
        "                self.weights[i] -= lr * m_W_nesterov / (np.sqrt(v_W_corrected) + epsilon)\n",
        "\n",
        "                # Same for biases\n",
        "                caches[\"nadam\"][\"m_b\"][i] = beta1 * caches[\"nadam\"][\"m_b\"][i] + (1 - beta1) * grads_b[i]\n",
        "                caches[\"nadam\"][\"v_b\"][i] = beta2 * caches[\"nadam\"][\"v_b\"][i] + (1 - beta2) * (grads_b[i]**2)\n",
        "                m_b_corrected = caches[\"nadam\"][\"m_b\"][i] / (1 - beta1**t)\n",
        "                v_b_corrected = caches[\"nadam\"][\"v_b\"][i] / (1 - beta2**t)\n",
        "                m_b_nesterov = beta1 * m_b_corrected + (1 - beta1) * grads_b[i] / (1 - beta1**t)\n",
        "                self.biases[i] -= lr * m_b_nesterov / (np.sqrt(v_b_corrected) + epsilon)\n",
        "\n",
        "        return caches\n",
        "\n",
        "# -----------------------\n",
        "# Utility functions\n",
        "def one_hot_encode(y, num_classes):\n",
        "    m = y.shape[0]\n",
        "    one_hot = np.zeros((m, num_classes))\n",
        "    one_hot[np.arange(m), y] = 1\n",
        "    return one_hot\n",
        "\n",
        "def compute_accuracy(Y_pred, Y_true):\n",
        "    pred_labels = np.argmax(Y_pred, axis=1)\n",
        "    true_labels = np.argmax(Y_true, axis=1)\n",
        "    return np.mean(pred_labels == true_labels)\n",
        "\n",
        "def plot_confusion_matrix(Y_pred, y_true, class_names=None):\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import seaborn as sns\n",
        "    cm = confusion_matrix(y_true, np.argmax(Y_pred, axis=1))\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------\n",
        "# Training function for wandb sweep agent\n",
        "def train():\n",
        "    # Initialize wandb run and read config values\n",
        "    wandb.init()\n",
        "    config = wandb.config\n",
        "\n",
        "    # Load the Fashion-MNIST dataset\n",
        "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "    X_train = X_train.reshape(X_train.shape[0], -1) / 255.\n",
        "    X_test = X_test.reshape(X_test.shape[0], -1) / 255.\n",
        "    num_classes = 10\n",
        "    y_train_oh = one_hot_encode(y_train, num_classes)\n",
        "    y_test_oh = one_hot_encode(y_test, num_classes)\n",
        "\n",
        "    # Split validation set (10% of training)\n",
        "    split = int(0.9 * X_train.shape[0])\n",
        "    X_val, y_val_oh = X_train[split:], y_train_oh[split:]\n",
        "    X_train, y_train_oh = X_train[:split], y_train_oh[:split]\n",
        "\n",
        "    # Build the model using hyperparameters from wandb.config\n",
        "    input_size = X_train.shape[1]\n",
        "    hidden_sizes = [config.hiddennodes] * config.hiddenlayers\n",
        "    model = FeedForwardNN(input_size, hidden_sizes, num_classes,\n",
        "                          activation=config.activation_func, initializer=config.initializer)\n",
        "\n",
        "    caches = {}  # For optimizer-specific states if needed\n",
        "    for epoch in range(config.num_epochs):\n",
        "        permutation = np.random.permutation(X_train.shape[0])\n",
        "        X_train = X_train[permutation]\n",
        "        y_train_oh = y_train_oh[permutation]\n",
        "        num_batches = X_train.shape[0] // config.batch_size\n",
        "        epoch_loss = 0\n",
        "        for i in range(num_batches):\n",
        "            start = i * config.batch_size\n",
        "            end = start + config.batch_size\n",
        "            X_batch = X_train[start:end]\n",
        "            y_batch = y_train_oh[start:end]\n",
        "            Y_pred = model.forward(X_batch)\n",
        "            loss = model.compute_loss(Y_pred, y_batch, loss_type=config.loss)\n",
        "            epoch_loss += loss\n",
        "            grads_W, grads_b = model.backprop(X_batch, y_batch, loss_type=config.loss)\n",
        "            caches = model.update_parameters(grads_W, grads_b, config.opt, config, caches)\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        val_pred = model.forward(X_val)\n",
        "        val_acc = compute_accuracy(val_pred, y_val_oh)\n",
        "        wandb.log({\"epoch\": epoch+1, \"loss\": avg_loss, \"val_accuracy\": val_acc})\n",
        "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Val Accuracy={val_acc:.4f}\")\n",
        "\n",
        "    test_pred = model.forward(X_test)\n",
        "    test_acc = compute_accuracy(test_pred, y_test_oh)\n",
        "    wandb.log({\"test_accuracy\": test_acc})\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    plot_confusion_matrix(test_pred, y_test, class_names=[str(i) for i in range(num_classes)])\n",
        "\n",
        "# -----------------------\n",
        "# Sweep configuration for wandb\n",
        "sweep_config = {\n",
        "    'name': \"karapa-rajesh\",\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "        'name': 'val_accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'hiddenlayers': {'values': [3, 4, 5]},\n",
        "        'num_epochs': {'values': [5, 10]},\n",
        "        'hiddennodes': {'values': [32, 64, 128]},\n",
        "        'learning_rate': {'values': [1e-3, 1e-4]},\n",
        "        'initializer': {'values': [\"random\", \"Xavier\"]},\n",
        "        'batch_size': {'values': [16, 32, 64]},\n",
        "        'opt': {'values': [\"sgd\", \"mbgd\", \"nesterov\", \"rmsprop\", \"adam\", \"nadam\"]},\n",
        "        'activation_func': {'values': [\"sigmoid\", \"tanh\", \"relu\"]},\n",
        "        'loss': {'values': [\"cross_entropy\", \"mean_squared_error\"]}\n",
        "    }\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# Main block to run either a single training run or a sweep agent.\n",
        "if __name__ == \"__main__\":\n",
        "    # To create a sweep (run once and note the sweep ID), uncomment the following:\n",
        "    #  sweep_id = wandb.sweep(sweep_config, project=\"DeepLearning\")\n",
        "    #  print(\"Sweep ID:\", sweep_id)\n",
        "\n",
        "    # For a standalone training run (without sweeps), uncomment the following:\n",
        "    # train()\n",
        "\n",
        "    # To run as a sweep agent, replace \"YOUR_SWEEP_ID_HERE\" with your actual sweep ID and run:\n",
        "    wandb.agent(\"ijfbxx3d\", function=train, count=5)"
      ]
    }
  ]
}